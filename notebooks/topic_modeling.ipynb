{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "I'd will use unsupervised learning methods and NLP techniques, leverage Spark to perform some analysis to understand why some [Mediun Blog Posts](https://medium.com/) are more popular than others. In fact, there are probably many factors that can contribute to why a blog post is more popular than others. For example, a blog post with a trendy topic, digestible and meaningful contents, a thesis that echos with majority of readers, ..., is probably a popular blog post. However, I will not explore every possible factor that moves the lever. I'd like to narrow down my research scope and explore a specific area, which is the \"topic\". I'd like to find out answers to two research questions:\n",
    "1. What \"latent topics\" do blog writers like to write about? In other words, what is the current trend of 'latent topics' in Midium blog posts' contents\n",
    "2. What \"latent topics\" are well-accepted and echo with majority of Medium readers?\n",
    "3. (Optional) Should blog post writers produce more blogs that cater readers' appetite? \n",
    "\n",
    "First of all, we need to define a blog post's \"popularity\" metric. I will use number of claps as the metric to indicate the level of \"popularity\" of a blog post.\n",
    "\n",
    "To understand what are the 'latent topics', we need to use [latent dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "\n",
    "## Data Sources\n",
    "There is not a single data science project without data. So I use [scrapy](https://scrapy.org/) to crawl pages on [Mediun Blog Posts](https://medium.com/), and scrap blog title, author, content, claps, and other information, then save them as raw data for research. Source code can be found on [Github](https://github.com/KevinLiao159/MediumBlog/tree/master/src). Due to my limited resources, I only sub-sample a few highlevel topics (data science, blockchain, artificial intelligence, startup, web development, software development) and only query blogs that were posted between 2018/01/01 and 2018/05/01\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. Load Data & Basic Data Cleaning \n",
    "2. Basic Exploratory Data Analysis<br/>\n",
    "    i. trends of different high-level topics<br/>\n",
    "    ii. high-level topic distribution<br/>\n",
    "    iii. high-level topics vs. claps<br/>\n",
    "3. Natural Language Processing<br/>\n",
    "    i. preprocess / clean text, tokenization, lemmetization/stemming<br/>\n",
    "    ii. TF-IDF vectorization<br/>\n",
    "    iii. K-means clustering<br/>\n",
    "    iv. LDA (Latent Dirichlet allocation)<br/>\n",
    "    v. validate models by visual displays with dimension reduction (PCA/T-SNE)<br/>\n",
    "4. Analysis on Blog Post Popularity vs. Latent Topics\n",
    "5. Analysis on Trends in Different Latent Topics\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark entry point\n",
    "from pyspark import SparkContext\n",
    "# dataframe / SQL entry point# datafr \n",
    "from pyspark.sql import SQLContext, SparkSession, DataFrame\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, ArrayType\n",
    "\n",
    "# other import\n",
    "import os\n",
    "from unicodedata import normalize\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from __future__ import division # for Python 2.x\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark config\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "sql_sc = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Basic Data Cleaning \n",
    "I use spark sql session to read json lines directly.\n",
    "1. read data in Spark engine\n",
    "2. normalize text\n",
    "3. convert data type to proper type\n",
    "4. filter date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    preprcess raw data\n",
    "    \n",
    "    input: spark dataframe\n",
    "    output: spark dataframe with proper dtypes and clean/normalized unicode string\n",
    "    \"\"\"\n",
    "    # create a user defined functon for apply method in spark dataframe\n",
    "    normalizer = UserDefinedFunction(lambda x: normalize('NFKD', x).replace(';', ' '), StringType())\n",
    "    # normalization and date filter\n",
    "    data = data.select(\n",
    "        data.publish_time.cast(TimestampType()),\n",
    "        normalizer(\"title\").alias(\"title\"),\n",
    "        normalizer(\"contents\").alias(\"contents\"),\n",
    "        data.claps.cast(IntegerType()),\n",
    "        data.tags.cast(ArrayType(StringType())),\n",
    "    )\n",
    "    return data.filter(data.publish_time > '2018-01-01').filter(data.publish_time < '2018-05-01')\n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    \"\"\"\n",
    "    union all tables into one table vertically\n",
    "    \"\"\"\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json lines file\n",
    "path_files = [os.path.join(data_path, f_name) for f_name in os.listdir(data_path) if f_name.endswith('.jl')]\n",
    "\n",
    "# init list of spark dataframes\n",
    "dfs = []\n",
    "for path in path_files:\n",
    "    # read json data into spark driver\n",
    "    data = spark.read.json(path)\n",
    "    # preprocess\n",
    "    data = preprocess(data)\n",
    "    # append to list\n",
    "    dfs.append(data)\n",
    "    \n",
    "# union all\n",
    "df = unionAll(*dfs).drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "|       publish_time|               title|            contents|claps|                tags|\n",
      "+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "|2018-01-08 00:00:00|12 Question Co-Fo...|When building a b...|   17|[Startup, Retrosp...|\n",
      "|2018-03-08 00:00:00|12 Startups in 12...|Few days ago duri...|    1|[Startup, Product...|\n",
      "|2018-03-08 00:00:00|17 Tips & Tricks ...|This is the third...|  202|[Freelancing, Sof...|\n",
      "|2018-03-27 00:00:00|3 Great Ways in W...|â€œIf you run a bus...|    0|[Web Development,...|\n",
      "|2018-03-17 00:00:00|4 Things To Keep ...|As you know web d...|    0|[Web Development,...|\n",
      "+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Data Analysis\n",
    "1. trends of different high-level topics\n",
    "2. high-level topic distribution\n",
    "3. high-level topics vs. claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
